{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42bc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48492d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746951e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('../data/reddit_ai.csv')\n",
    "youtube_df = pd.read_csv('../data/youtube_ai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be824e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_reddit_data(df):\n",
    "    \"\"\"Standardize Reddit data to unified schema\"\"\"\n",
    "    \n",
    "    # Create unified schema\n",
    "    harmonized = pd.DataFrame()\n",
    "    \n",
    "    # Core text fields\n",
    "    harmonized['text'] = df['text']\n",
    "    harmonized['text_length'] = df['comment_length']\n",
    "    \n",
    "    # Sentiment\n",
    "    harmonized['sentiment_polarity'] = df['sentiment_polarity']\n",
    "    harmonized['sentiment_subjectivity'] = df['sentiment_subjectivity']\n",
    "    harmonized['sentiment_label'] = df.get('sentiment_label', \n",
    "                                            df['sentiment_polarity'].apply(\n",
    "                                                lambda x: 'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral')))\n",
    "    \n",
    "    # Engagement metrics\n",
    "    harmonized['likes'] = df['likes']\n",
    "    harmonized['replies'] = df['num_replies']\n",
    "    \n",
    "    # Temporal\n",
    "    harmonized['created_at'] = pd.to_datetime(df['created_utc'], errors='coerce')\n",
    "    \n",
    "    # Source identification\n",
    "    harmonized['source'] = 'reddit'\n",
    "    harmonized['source_id'] = df['source_id']\n",
    "    \n",
    "    # Platform-specific metadata\n",
    "    harmonized['platform_post_id'] = df['post_id']\n",
    "    harmonized['platform_post_title'] = df['post_title']\n",
    "    harmonized['platform_community'] = df['subreddit']\n",
    "    harmonized['platform_post_score'] = df['post_score']\n",
    "    harmonized['platform_post_engagement'] = df['post_num_comments']\n",
    "    \n",
    "    # Content flags\n",
    "    harmonized['contains_ai'] = df['contains_ai']\n",
    "    harmonized['contains_opinion'] = df['contains_opinion']\n",
    "    harmonized['contains_societal'] = df['contains_societal']\n",
    "    \n",
    "    return harmonized\n",
    "\n",
    "def harmonize_youtube_data(df):\n",
    "    \"\"\"Standardize YouTube data to unified schema\"\"\"\n",
    "    \n",
    "    # Create unified schema\n",
    "    harmonized = pd.DataFrame()\n",
    "    \n",
    "    # Core text fields\n",
    "    harmonized['text'] = df['text']\n",
    "    harmonized['text_length'] = df['comment_length']\n",
    "    \n",
    "    # Sentiment\n",
    "    harmonized['sentiment_polarity'] = df['sentiment_polarity']\n",
    "    harmonized['sentiment_subjectivity'] = df['sentiment_subjectivity']\n",
    "    harmonized['sentiment_label'] = df.get('sentiment_label',\n",
    "                                            df['sentiment_polarity'].apply(\n",
    "                                                lambda x: 'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral')))\n",
    "    \n",
    "    # Engagement metrics\n",
    "    harmonized['likes'] = df['likes']\n",
    "    harmonized['replies'] = df['num_replies']\n",
    "    \n",
    "    # Temporal\n",
    "    harmonized['created_at'] = pd.to_datetime(df['created_utc'], errors='coerce')\n",
    "    \n",
    "    # Source identification\n",
    "    harmonized['source'] = 'youtube'\n",
    "    harmonized['source_id'] = df['source_id']\n",
    "    \n",
    "    # Platform-specific metadata\n",
    "    harmonized['platform_post_id'] = df['video_id']\n",
    "    harmonized['platform_post_title'] = df['video_title']\n",
    "    harmonized['platform_community'] = df['video_channel']\n",
    "    harmonized['platform_post_score'] = df['video_like_count']\n",
    "    harmonized['platform_post_engagement'] = df['video_comment_count']\n",
    "    \n",
    "    # Content flags\n",
    "    harmonized['contains_ai'] = df['contains_ai']\n",
    "    harmonized['contains_opinion'] = df['contains_opinion']\n",
    "    harmonized['contains_societal'] = df['contains_societal']\n",
    "    \n",
    "    return harmonized\n",
    "\n",
    "# Harmonize datasets\n",
    "reddit_harmonized = harmonize_reddit_data(reddit_df)\n",
    "youtube_harmonized = harmonize_youtube_data(youtube_df)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.concat([reddit_harmonized, youtube_harmonized], ignore_index=True)\n",
    "merged_df.to_csv('../data/ai_opinions_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d1ae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9de016bc-34c7-4825-8759-ba8c8b583cf4",
       "rows": [
        [
         "text",
         "0"
        ],
        [
         "text_length",
         "0"
        ],
        [
         "sentiment_polarity",
         "0"
        ],
        [
         "sentiment_subjectivity",
         "0"
        ],
        [
         "sentiment_label",
         "0"
        ],
        [
         "likes",
         "0"
        ],
        [
         "replies",
         "0"
        ],
        [
         "created_at",
         "0"
        ],
        [
         "source",
         "0"
        ],
        [
         "source_id",
         "0"
        ],
        [
         "platform_post_id",
         "0"
        ],
        [
         "platform_post_title",
         "0"
        ],
        [
         "platform_community",
         "0"
        ],
        [
         "platform_post_score",
         "0"
        ],
        [
         "platform_post_engagement",
         "0"
        ],
        [
         "contains_ai",
         "0"
        ],
        [
         "contains_opinion",
         "0"
        ],
        [
         "contains_societal",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 18
       }
      },
      "text/plain": [
       "text                        0\n",
       "text_length                 0\n",
       "sentiment_polarity          0\n",
       "sentiment_subjectivity      0\n",
       "sentiment_label             0\n",
       "likes                       0\n",
       "replies                     0\n",
       "created_at                  0\n",
       "source                      0\n",
       "source_id                   0\n",
       "platform_post_id            0\n",
       "platform_post_title         0\n",
       "platform_community          0\n",
       "platform_post_score         0\n",
       "platform_post_engagement    0\n",
       "contains_ai                 0\n",
       "contains_opinion            0\n",
       "contains_societal           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f34365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Cleaning for TF-IDF (aggressive preprocessing)...\n",
      "2. Cleaning for BERT (minimal preprocessing)...\n",
      "3. Creating basic cleaned version for EDA...\n",
      "\n",
      "✓ Text cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "## 6. Text Cleaning Functions\n",
    "\n",
    "class TextCleaner:\n",
    "    \"\"\"Comprehensive text cleaning and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Custom patterns\n",
    "        self.url_pattern = re.compile(r'http\\S+|www\\.\\S+|https\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+')\n",
    "        self.mention_pattern = re.compile(r'@\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\w+')\n",
    "        self.number_pattern = re.compile(r'\\d+')\n",
    "        \n",
    "    def remove_urls(self, text):\n",
    "        \"\"\"Remove URLs\"\"\"\n",
    "        return self.url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        \"\"\"Remove email addresses\"\"\"\n",
    "        return self.email_pattern.sub('', text)\n",
    "    \n",
    "    def remove_mentions(self, text):\n",
    "        \"\"\"Remove social media mentions\"\"\"\n",
    "        return self.mention_pattern.sub('', text)\n",
    "    \n",
    "    def remove_hashtags(self, text):\n",
    "        \"\"\"Remove hashtags but keep the text\"\"\"\n",
    "        return self.hashtag_pattern.sub(lambda m: m.group(0)[1:], text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text):\n",
    "        \"\"\"Remove extra whitespace\"\"\"\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def lowercase(self, text):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation(self, text, keep_sentence_end=True):\n",
    "        \"\"\"Remove punctuation, optionally keep sentence endings\"\"\"\n",
    "        if keep_sentence_end:\n",
    "            # Keep . ! ? for sentence structure\n",
    "            translator = str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', ''))\n",
    "        else:\n",
    "            translator = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(translator)\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        \"\"\"Remove numbers\"\"\"\n",
    "        return self.number_pattern.sub('', text)\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        filtered = [w for w in words if w.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered)\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"Lemmatize text\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        lemmatized = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "        return ' '.join(lemmatized)\n",
    "    \n",
    "    def stem(self, text):\n",
    "        \"\"\"Stem text\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        stemmed = [self.stemmer.stem(w) for w in words]\n",
    "        return ' '.join(stemmed)\n",
    "    \n",
    "    def clean_basic(self, text):\n",
    "        \"\"\"Basic cleaning: URLs, emails, whitespace, lowercase\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = self.remove_urls(text)\n",
    "        text = self.remove_emails(text)\n",
    "        text = self.remove_mentions(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def clean_standard(self, text):\n",
    "        \"\"\"Standard cleaning: basic + punctuation + numbers\"\"\"\n",
    "        text = self.clean_basic(text)\n",
    "        text = self.remove_punctuation(text, keep_sentence_end=False)\n",
    "        text = self.remove_numbers(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def clean_aggressive(self, text):\n",
    "        \"\"\"Aggressive cleaning: standard + stopwords + lemmatization\"\"\"\n",
    "        text = self.clean_standard(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.lemmatize(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "## 7. Apply Text Cleaning (TF-IDF vs BERT Optimized)\n",
    "\n",
    "def clean_for_tfidf(text, cleaner):\n",
    "    \"\"\"\n",
    "    Optimized cleaning for TF-IDF:\n",
    "    - Aggressive preprocessing: lowercase, no punctuation, no stopwords\n",
    "    - Lemmatization to reduce vocabulary\n",
    "    - Remove numbers (they're not meaningful for TF-IDF)\n",
    "    - Goal: Clean bag-of-words representation\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = cleaner.remove_urls(text)\n",
    "    text = cleaner.remove_emails(text)\n",
    "    text = cleaner.remove_mentions(text)\n",
    "    text = cleaner.remove_hashtags(text)\n",
    "    text = cleaner.lowercase(text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = cleaner.remove_punctuation(text, keep_sentence_end=False)\n",
    "    text = cleaner.remove_numbers(text)\n",
    "    \n",
    "    # Remove stopwords (they don't add value to TF-IDF)\n",
    "    text = cleaner.remove_stopwords(text)\n",
    "    \n",
    "    # Lemmatize to reduce vocabulary size\n",
    "    text = cleaner.lemmatize(text)\n",
    "    \n",
    "    # Clean whitespace\n",
    "    text = cleaner.remove_extra_whitespace(text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_for_bert(text, cleaner):\n",
    "    \"\"\"\n",
    "    Optimized cleaning for BERT:\n",
    "    - Minimal preprocessing: preserve context and semantics\n",
    "    - Keep punctuation (BERT uses it for understanding)\n",
    "    - Keep stopwords (BERT learns from them)\n",
    "    - Keep case variations (BERT has case-sensitive and case-insensitive versions)\n",
    "    - Keep numbers (can be contextually important)\n",
    "    - Goal: Natural, contextual text that BERT can understand\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Only remove noise that doesn't add meaning\n",
    "    text = cleaner.remove_urls(text)\n",
    "    text = cleaner.remove_emails(text)\n",
    "    \n",
    "    # Keep mentions and hashtags as they might have semantic value\n",
    "    # Just remove the @ and # symbols\n",
    "    text = text.replace('@', '').replace('#', '')\n",
    "    \n",
    "    # Clean extra whitespace but preserve structure\n",
    "    text = cleaner.remove_extra_whitespace(text)\n",
    "    \n",
    "    # Keep original case, punctuation, numbers, stopwords\n",
    "    # BERT's tokenizer will handle these appropriately\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# TF-IDF optimized: aggressive cleaning\n",
    "print(\"\\n1. Cleaning for TF-IDF (aggressive preprocessing)...\")\n",
    "merged_df['text_tfidf'] = merged_df['text'].apply(lambda x: clean_for_tfidf(x, cleaner))\n",
    "\n",
    "# BERT optimized: minimal cleaning\n",
    "print(\"2. Cleaning for BERT (minimal preprocessing)...\")\n",
    "merged_df['text_bert'] = merged_df['text'].apply(lambda x: clean_for_bert(x, cleaner))\n",
    "\n",
    "# Also keep a basic cleaned version for general analysis\n",
    "print(\"3. Creating basic cleaned version for EDA...\")\n",
    "merged_df['text_clean'] = merged_df['text'].apply(cleaner.clean_basic)\n",
    "\n",
    "print(\"\\n✓ Text cleaning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a43f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create additional features for analysis\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- Temporal Features ---\n",
    "    df['year'] = df['created_at'].dt.year\n",
    "    df['month'] = df['created_at'].dt.month\n",
    "    df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "    df['hour'] = df['created_at'].dt.hour\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # --- Text Features (using basic clean for consistency) ---\n",
    "    df['word_count'] = df['text_clean'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Punctuation features\n",
    "    df['exclamation_count'] = df['text'].apply(lambda x: str(x).count('!'))\n",
    "    df['question_count'] = df['text'].apply(lambda x: str(x).count('?'))\n",
    "    \n",
    "    # --- Engagement Features ---\n",
    "    df['engagement_score'] = df['likes'] + (df['replies'] * 2)  # Weight replies higher\n",
    "    \n",
    "    # Log transform for skewed features\n",
    "    df['likes_log'] = np.log1p(df['likes'])\n",
    "    df['replies_log'] = np.log1p(df['replies'])\n",
    "    df['engagement_log'] = np.log1p(df['engagement_score'])\n",
    "    \n",
    "    # --- Sentiment Features ---\n",
    "    df['is_positive'] = (df['sentiment_polarity'] > 0.1).astype(int)\n",
    "    df['is_negative'] = (df['sentiment_polarity'] < -0.1).astype(int)\n",
    "    df['is_neutral'] = ((df['sentiment_polarity'] >= -0.1) & (df['sentiment_polarity'] <= 0.1)).astype(int)\n",
    "    df['sentiment_magnitude'] = np.abs(df['sentiment_polarity'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c49aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = engineer_features(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63522e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('../data/df_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
