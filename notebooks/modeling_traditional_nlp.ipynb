{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa185da",
   "metadata": {},
   "source": [
    "## Traditional Methods of Feature engineering and ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4536822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/reddit_ai_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6766268a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "selftext_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upvote_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_comments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "post_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sentiment_subjectivity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "created_datetime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "high_score",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "bdd80284-70db-4233-8aa9-1dfb56c6d188",
       "rows": [
        [
         "0",
         "How to retrieve instructions given to annotators - RLHF",
         "Hello, I am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis. The topic of my thesis is Human-induced communication and intercultural biases in LLMs the consequences of RLHF models. The data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process. My goal is to analyze these different instructions, coming from different providers nationalities, to see if the way these instructions are constructed can influence LLM learning. According to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology. Is contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used? Thank you very much for taking the time to respond, and for your answers! Have a great day.",
         "MachineLearning",
         "13",
         "0.93",
         "7",
         "171",
         "0.0958333333333333",
         "0.4043333333333334",
         "2025-10-10 08:49:17",
         "2025-10-10",
         "2025",
         "How to retrieve instructions given to annotators - RLHF Hello, I am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis. The topic of my thesis is Human-induced communication and intercultural biases in LLMs the consequences of RLHF models. The data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process. My goal is to analyze these different instructions, coming from different providers nationalities, to see if the way these instructions are constructed can influence LLM learning. According to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology. Is contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used? Thank you very much for taking the time to respond, and for your answers! Have a great day.",
         "1"
        ],
        [
         "1",
         "Built an ML-based Variant Impact Predictor non-deep learning for genomic variant prioritization",
         "Hey folks, I ve been working on a small ML project over the last month and thought it might interest some of you doing variant analysis or functional genomics. It s a non-deep-learning model Gradient Boosting Random Forests that predicts the functional impact of genetic variants SNPs, indels using public annotations like ClinVar, gnomAD, Ensembl, and UniProt features. The goal is to help filter or prioritize variants before downstream experiments for example ranking variants from a new sequencing project, triaging variants of unknown significance, or focusing on variants likely to alter protein function. The model uses features like conservation scores PhyloP, PhastCons , allele frequencies, functional class missense, nonsense, etc. , gene constraint metrics like pLI , and pre-existing scores SIFT, PolyPhen2, etc. . I kept it deliberately lightweight runs easily on Colab, no GPUs, and trains on openly available variant data. It s designed for research-use-only and doesn t attempt any clinical classification. I d love to hear feedback from others working on ML in genomics particularly about useful features to include, ways to benchmark, or datasets worth adding. If anyone s curious about using a version of it internally e.g., for variant triage in a research setting , you can DM me for details about the commercial license. Happy to discuss technical stuff openly in the thread I m mostly sharing this because it s been fun applying classical ML to genomics in a practical way",
         "MachineLearning",
         "0",
         "0.4",
         "10",
         "244",
         "0.131198347107438",
         "0.393388429752066",
         "2025-10-09 18:40:15",
         "2025-10-09",
         "2025",
         "Built an ML-based Variant Impact Predictor non-deep learning for genomic variant prioritization Hey folks, I ve been working on a small ML project over the last month and thought it might interest some of you doing variant analysis or functional genomics. It s a non-deep-learning model Gradient Boosting Random Forests that predicts the functional impact of genetic variants SNPs, indels using public annotations like ClinVar, gnomAD, Ensembl, and UniProt features. The goal is to help filter or prioritize variants before downstream experiments for example ranking variants from a new sequencing project, triaging variants of unknown significance, or focusing on variants likely to alter protein function. The model uses features like conservation scores PhyloP, PhastCons , allele frequencies, functional class missense, nonsense, etc. , gene constraint metrics like pLI , and pre-existing scores SIFT, PolyPhen2, etc. . I kept it deliberately lightweight runs easily on Colab, no GPUs, and trains on openly available variant data. It s designed for research-use-only and doesn t attempt any clinical classification. I d love to hear feedback from others working on ML in genomics particularly about useful features to include, ways to benchmark, or datasets worth adding. If anyone s curious about using a version of it internally e.g., for variant triage in a research setting , you can DM me for details about the commercial license. Happy to discuss technical stuff openly in the thread I m mostly sharing this because it s been fun applying classical ML to genomics in a practical way",
         "0"
        ],
        [
         "2",
         "Tensorflow and Musicnn",
         "Hi all, I m struggling with Tensorflow and an old Musicnn embbeding and classification model that I get form the Essentia project. To say in short seems that in same CPU it doesn t work. Initially I collect issue on old CPU due to the missing support of AVX, and I can live with the fact of not support very old CPU. Now I discovered that also some not old cpu have some different rappresentation of number that broke the model with some memory error. The first issue that i fix was this It was an intel i5 1035G1 processor that by default used float64 instead of the float32 used by the model. Just adding a cast in my code I solved the problem, good. Some days ago an user with an AMD Ryzen AI 9 HX 370 had similar problem here I try to check if I miss some cast somewhere but I wasn t able to find a solution in that way. I instead found that by setting this env variable ENV TF ENABLE ONEDNN OPTS 0 The model start working but giving correct value but with a different scale. So the probability of a tag the genre of the song instead of be around 0.1 or 0.2 arrived to 0.5 or 0.6. So here my question why? How can achieve that Tensorflow work on different CPU and possibly giving similar value? I think can be ok if the precision is not the exact one, but have the double or the triple of the value to me sounds strange and I don t know which impact can have on the rest of my application. I mainly use The Musicnn embbeding rappresentation to do similarity song between embbeding itself. Then I use for a secondary purpose the tag itself with the genre. Any suggestion ? Eventually any good alternative to Tensorflow at all that could be more stable and that I can use in python ? My entire app is in python . Just for background the entire app is opensource and free on GitHub. If you want to inspect the code it is in task analysis all the part that use Librosa Tensorflow for this analysis yes the model was from Essentia, but I m reusing reading the song with Librosa because seems more updated and support ARM on Linux .",
         "MachineLearning",
         "1",
         "0.57",
         "11",
         "386",
         "0.1357429130009775",
         "0.4048924731182796",
         "2025-10-06 07:19:05",
         "2025-10-06",
         "2025",
         "Tensorflow and Musicnn Hi all, I m struggling with Tensorflow and an old Musicnn embbeding and classification model that I get form the Essentia project. To say in short seems that in same CPU it doesn t work. Initially I collect issue on old CPU due to the missing support of AVX, and I can live with the fact of not support very old CPU. Now I discovered that also some not old cpu have some different rappresentation of number that broke the model with some memory error. The first issue that i fix was this It was an intel i5 1035G1 processor that by default used float64 instead of the float32 used by the model. Just adding a cast in my code I solved the problem, good. Some days ago an user with an AMD Ryzen AI 9 HX 370 had similar problem here I try to check if I miss some cast somewhere but I wasn t able to find a solution in that way. I instead found that by setting this env variable ENV TF ENABLE ONEDNN OPTS 0 The model start working but giving correct value but with a different scale. So the probability of a tag the genre of the song instead of be around 0.1 or 0.2 arrived to 0.5 or 0.6. So here my question why? How can achieve that Tensorflow work on different CPU and possibly giving similar value? I think can be ok if the precision is not the exact one, but have the double or the triple of the value to me sounds strange and I don t know which impact can have on the rest of my application. I mainly use The Musicnn embbeding rappresentation to do similarity song between embbeding itself. Then I use for a secondary purpose the tag itself with the genre. Any suggestion ? Eventually any good alternative to Tensorflow at all that could be more stable and that I can use in python ? My entire app is in python . Just for background the entire app is opensource and free on GitHub. If you want to inspect the code it is in task analysis all the part that use Librosa Tensorflow for this analysis yes the model was from Essentia, but I m reusing reading the song with Librosa because seems more updated and support ARM on Linux .",
         "0"
        ],
        [
         "3",
         "Experiences with active learning for real applications?",
         "I'm tinkering with an application of human pose estimation which fails miserably. I've always been intrigued by active learning, so I'm looking forward to applying it here to efficiently sample frames for manual labeling. But I've never witnessed it in industry, and have only ever encountered pessimistic takes on active learning in general. As an extra layer of complexity - it seems like a manual labeler likely myself would have to enter labels through a browser GUI. Ideally, the labeler should produce labels concurrently as the model trains on its labels-thus-far and considers unlabeled frames to send to the labeler. Suddenly my training pipeline gets complicated! My current plan Sample training frames for labeling according to variance in predictions between adjacent frames, or perhaps dropout uncertainty. Higher uncertainty should -- worse predictions For the holdout val test sets split by video , sample frames truly at random In the labeling GUI, display the model's initial prediction, and just drag the skeleton around Don't bother with concurrent labeling training, way too much work. I care more about hours spent labeling than calendar time at this point. I'd love to know whether it's worth all the fuss. I'm curious to hear about any cases where active learning succeeded or flopped in an industry applied setting. In practice, when does active learning give a clear win over random? When will it probably be murkier? Recommended batch sizes cadence and stopping criteria? Common pitfalls uncertainty miscalibration, sampling bias, annotator fatigue ?",
         "MachineLearning",
         "4",
         "0.83",
         "6",
         "310",
         "-0.0255050505050505",
         "0.501641414141414",
         "2025-10-04 21:53:21",
         "2025-10-04",
         "2025",
         "Experiences with active learning for real applications? I'm tinkering with an application of human pose estimation which fails miserably. I've always been intrigued by active learning, so I'm looking forward to applying it here to efficiently sample frames for manual labeling. But I've never witnessed it in industry, and have only ever encountered pessimistic takes on active learning in general. As an extra layer of complexity - it seems like a manual labeler likely myself would have to enter labels through a browser GUI. Ideally, the labeler should produce labels concurrently as the model trains on its labels-thus-far and considers unlabeled frames to send to the labeler. Suddenly my training pipeline gets complicated! My current plan Sample training frames for labeling according to variance in predictions between adjacent frames, or perhaps dropout uncertainty. Higher uncertainty should -- worse predictions For the holdout val test sets split by video , sample frames truly at random In the labeling GUI, display the model's initial prediction, and just drag the skeleton around Don't bother with concurrent labeling training, way too much work. I care more about hours spent labeling than calendar time at this point. I'd love to know whether it's worth all the fuss. I'm curious to hear about any cases where active learning succeeded or flopped in an industry applied setting. In practice, when does active learning give a clear win over random? When will it probably be murkier? Recommended batch sizes cadence and stopping criteria? Common pitfalls uncertainty miscalibration, sampling bias, annotator fatigue ?",
         "0"
        ],
        [
         "4",
         "Thesis direction mechanistic interpretability vs semantic probing of LLM reasoning?",
         "Hi all, I'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions Mechanistic interpretability low-level reverse engineering smaller neural networks, analyzing weights activations, simple logic gates, and tracking learning dynamics. Semantic probing high-level designing behavioral tasks for LLMs, probing reasoning, attention locality, and consistency of inference. For context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack frontend at first, but my long-term goal is to transition into backend. I'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact? Thanks in advance for your advice!",
         "MachineLearning",
         "12",
         "0.8",
         "13",
         "165",
         "0.2193181818181817",
         "0.4502705627705628",
         "2025-10-02 20:50:44",
         "2025-10-02",
         "2025",
         "Thesis direction mechanistic interpretability vs semantic probing of LLM reasoning? Hi all, I'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions Mechanistic interpretability low-level reverse engineering smaller neural networks, analyzing weights activations, simple logic gates, and tracking learning dynamics. Semantic probing high-level designing behavioral tasks for LLMs, probing reasoning, attention locality, and consistency of inference. For context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack frontend at first, but my long-term goal is to transition into backend. I'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact? Thanks in advance for your advice!",
         "1"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_length</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to retrieve instructions given to annotato...</td>\n",
       "      <td>Hello, I am a communications student, and as p...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7</td>\n",
       "      <td>171</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.404333</td>\n",
       "      <td>2025-10-10 08:49:17</td>\n",
       "      <td>2025-10-10</td>\n",
       "      <td>2025</td>\n",
       "      <td>How to retrieve instructions given to annotato...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Built an ML-based Variant Impact Predictor non...</td>\n",
       "      <td>Hey folks, I ve been working on a small ML pro...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>10</td>\n",
       "      <td>244</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>0.393388</td>\n",
       "      <td>2025-10-09 18:40:15</td>\n",
       "      <td>2025-10-09</td>\n",
       "      <td>2025</td>\n",
       "      <td>Built an ML-based Variant Impact Predictor non...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tensorflow and Musicnn</td>\n",
       "      <td>Hi all, I m struggling with Tensorflow and an ...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11</td>\n",
       "      <td>386</td>\n",
       "      <td>0.135743</td>\n",
       "      <td>0.404892</td>\n",
       "      <td>2025-10-06 07:19:05</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>Tensorflow and Musicnn Hi all, I m struggling ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experiences with active learning for real appl...</td>\n",
       "      <td>I'm tinkering with an application of human pos...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>4</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6</td>\n",
       "      <td>310</td>\n",
       "      <td>-0.025505</td>\n",
       "      <td>0.501641</td>\n",
       "      <td>2025-10-04 21:53:21</td>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>2025</td>\n",
       "      <td>Experiences with active learning for real appl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thesis direction mechanistic interpretability ...</td>\n",
       "      <td>Hi all, I'm an undergrad Computer Science stud...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>12</td>\n",
       "      <td>0.80</td>\n",
       "      <td>13</td>\n",
       "      <td>165</td>\n",
       "      <td>0.219318</td>\n",
       "      <td>0.450271</td>\n",
       "      <td>2025-10-02 20:50:44</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>2025</td>\n",
       "      <td>Thesis direction mechanistic interpretability ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_clean  \\\n",
       "0  How to retrieve instructions given to annotato...   \n",
       "1  Built an ML-based Variant Impact Predictor non...   \n",
       "2                             Tensorflow and Musicnn   \n",
       "3  Experiences with active learning for real appl...   \n",
       "4  Thesis direction mechanistic interpretability ...   \n",
       "\n",
       "                                      selftext_clean        subreddit  score  \\\n",
       "0  Hello, I am a communications student, and as p...  MachineLearning     13   \n",
       "1  Hey folks, I ve been working on a small ML pro...  MachineLearning      0   \n",
       "2  Hi all, I m struggling with Tensorflow and an ...  MachineLearning      1   \n",
       "3  I'm tinkering with an application of human pos...  MachineLearning      4   \n",
       "4  Hi all, I'm an undergrad Computer Science stud...  MachineLearning     12   \n",
       "\n",
       "   upvote_ratio  num_comments  post_length  sentiment_polarity  \\\n",
       "0          0.93             7          171            0.095833   \n",
       "1          0.40            10          244            0.131198   \n",
       "2          0.57            11          386            0.135743   \n",
       "3          0.83             6          310           -0.025505   \n",
       "4          0.80            13          165            0.219318   \n",
       "\n",
       "   sentiment_subjectivity     created_datetime        date  year  \\\n",
       "0                0.404333  2025-10-10 08:49:17  2025-10-10  2025   \n",
       "1                0.393388  2025-10-09 18:40:15  2025-10-09  2025   \n",
       "2                0.404892  2025-10-06 07:19:05  2025-10-06  2025   \n",
       "3                0.501641  2025-10-04 21:53:21  2025-10-04  2025   \n",
       "4                0.450271  2025-10-02 20:50:44  2025-10-02  2025   \n",
       "\n",
       "                                                text  high_score  \n",
       "0  How to retrieve instructions given to annotato...           1  \n",
       "1  Built an ML-based Variant Impact Predictor non...           0  \n",
       "2  Tensorflow and Musicnn Hi all, I m struggling ...           0  \n",
       "3  Experiences with active learning for real appl...           0  \n",
       "4  Thesis direction mechanistic interpretability ...           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add bianry label to predict which posts will get high engagement\n",
    "df[\"high_score\"] = (df[\"score\"] >= df[\"score\"].median()).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74a2c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('numeric',\n",
       " StandardScaler(),\n",
       " ['post_length', 'sentiment_polarity', 'sentiment_subjectivity'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(\"numeric\", StandardScaler(), [\"post_length\", \"sentiment_polarity\", \"sentiment_subjectivity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec010ee2",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040ef32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.50      0.60        52\n",
      "           1       0.55      0.78      0.65        41\n",
      "\n",
      "    accuracy                           0.62        93\n",
      "   macro avg       0.65      0.64      0.62        93\n",
      "weighted avg       0.66      0.62      0.62        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "X = df[[\n",
    "    \"title_clean\",\n",
    "    \"selftext_clean\",\n",
    "    \"post_length\",\n",
    "    \"sentiment_polarity\",\n",
    "    \"sentiment_subjectivity\"\n",
    "]]\n",
    "y = df[\"high_score\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.90,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=\"english\",\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tfidf_title\", tfidf, \"title_clean\"),\n",
    "        (\"tfidf_text\", tfidf, \"selftext_clean\"),\n",
    "        (\"numeric\", StandardScaler(), [\"post_length\",\n",
    "                                       \"sentiment_polarity\",\n",
    "                                       \"sentiment_subjectivity\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf20fd1",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b29e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67        52\n",
      "           1       0.60      0.68      0.64        41\n",
      "\n",
      "    accuracy                           0.66        93\n",
      "   macro avg       0.66      0.66      0.65        93\n",
      "weighted avg       0.66      0.66      0.66        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", max_iter=5000, C=0.5))\n",
    "])\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "preds = model2.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4023",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5734804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a043a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'clf__n_estimators': 200, 'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 0.5, 'clf__max_depth': 20}\n",
      "Best CV score: 0.559738198339088\n",
      "\n",
      "Tuned Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.64        52\n",
      "           1       0.58      0.78      0.67        41\n",
      "\n",
      "    accuracy                           0.66        93\n",
      "   macro avg       0.67      0.67      0.66        93\n",
      "weighted avg       0.68      0.66      0.65        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 2: With hyperparameter tuning\n",
    "model_rf_tuned = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__max_depth': [None, 10, 20, 30],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__max_features': ['sqrt', 'log2', 0.5]  # Important for high-dimensional TF-IDF\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning (or GridSearchCV for exhaustive)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    model_rf_tuned,\n",
    "    param_grid,\n",
    "    n_iter=20,  # Number of parameter settings to sample\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", rf_search.best_params_)\n",
    "print(\"Best CV score:\", rf_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = rf_search.best_estimator_\n",
    "preds_rf_best = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\nTuned Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, preds_rf_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97a25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
